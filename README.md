# IPv6 Socket Programming

### 2021년 2학기 최형기 교수님 컴퓨터네트웍개론 프로젝트

- Socket programming을 통해 IPv4 및 IPv6 통신 구현

	1. [`IPv4 클라이언트`로 IPv4 서버에 접속하여 자신의 IPv6 서버 IP 및 포트번호 전달](#1-ipv4-client가-ipv4-서버에-접속)

	2. [`IPv6 서버`는 concurrent하게 구현되어 IPv6 클라이언트(총 5개, multi clients)가 전달하는 난수들을 수신 후, 이를 `IPv4 클라이언트`에 전달](#2-ipv6-서버가-multi-clients로부터-5개의-토큰을-수신-후-전달)

	3. [`IPv4 클라이언트`는 전달받은 토큰 5개를 IPv4 서버에 전송](#3-ipv4-클라이언트가-전달받은-토큰-5개를-ipv4-서버에-전송)

- multi clients에 대응하는 concurrent 서버 구현 필요

- IPv6 서버를 위한 tunneling 필요

## 1. Protocol and Program details
<img width="85%" src="https://user-images.githubusercontent.com/94846990/144199378-92fcc5a1-3638-4cf2-9fd8-86fe0d000d02.png">

<br/>

## 2. Used Environment
- Host OS: macOS Monterey 12.0.1
- Guest OS: Ubuntu 20.0 on VMware Fusion
- Language: C
- IDE: VScode on Ubuntu (20.04)

<br/>

## 3. Programming
### 1\) IPv4 client가 IPv4 서버에 접속
- **IPv6 서버 IP 및 포트번호 전달**
	- 예시로 주어진 example.pcap 파일을 확인해보면, 먼저 터미널 상의 색을 지정하는 패킷이 한 번 수신된 다음 메세지가 전송됨. 따라서 패킷을 두 번 연속으로 읽어와야 할 것
	- 주어진 문항에 답변하는 패킷을 보낼 때, `scanf()`를 사용하면 개행문자까지 버퍼에 저장되고, 이 함수는 개행문자 직전까지만 버퍼에서 읽어들여 변수에 저장하므로 다음번 문항에 답변하려 할 때 버퍼에 남아있던 개행문자로 인해 사용자의 입력을 받지 않고 넘어가는 문제가 발생할 것
		- `read()`함수를 사용하여 파일 디스크립터 인자로 `0`을 주면 표준 입력으로부터 읽어들이므로 해결 가능
	- `read()` 함수를 통해 파일의 len 바이트 만큼 읽으라고 요청하였지만 읽은 데이터가 없다면, `read()`는 읽은 바이트가 생길 때 까지 `block`된다. 이 시스템 콜로 인한 블록은 곧 프로세스가 블록됨을 의미하므로, IPv4 클라이언트는 `fork()` 등의 방법을 사용하지 않고 단일 프로세스로 작동되고 있음을 고려했을 때 `read()` 아랫부분의 코드는 작동하지 않고 같이 블록된다. 즉 `read()` 함수를 무분별하게 쓰면 프로그램이 중간에 멈춘 것 처럼 보일 것.

### 2\) IPv6 서버가 multi clients로부터 5개의 토큰을 수신 후 전달
**(1) Concurrent IPv6 서버 구현**
- concurrent하게 구현하지 않으면 `accept()` 요청 자체가 수신되지 않음
- 자식 프로세스가 읽어들인 토큰 값을 부모 프로세스에게 전달하는 IPC 통신 필요
	- 자식 프로세스가 데이터를 전달하면 부모 프로세스가 읽어들이는 단방향 통신이므로 간단하게 `PIPE` 방식 사용

**(2) IPv6 서버와 IPv4 클라이언트 간 통신**
- 앞서 IPv6 서버 내에서의 IPC와 다르게 IPv6 서버와 IPv4 클라이언트 간 IPC의 경우 두 프로세스의 PPID가 다르므로 다른 방식의 IPC 필요
- `fopen()` 등의 방식으로 파일을 생성해서 쓰고 읽는 방식의 통신은 금지됨
	- 앞선 `PIPE` 방식의 경우 이름을 붙이면 (`named PIPE`) PPID를 공유하지 않는 프로세스 간 통신도 가능하지만, `PIPE` 방식 자체가 `temporal FIFO 파일`을 만들어 파일 디스크립터를 통해 접근 및 읽고 쓰는 방식이므로 `PIPE` 대신 `Message Queue` 방식을 사용함
- **`Message Queue` 방식**: 위에서 IPv6 서버 내 부모-자식 프로세스 간 통신을 구현하는 데 사용한 PIPE는 일반 PIPE와 Named PIPE로 다시 나뉘는데, Named PIPE의 경우 mkfifo를 통해 이름이 있는 파일을 사용하기 때문에 앞선 PIPE와 달리 다른 모든 프로세스들 간에 통신이 가능합니다. 그러나 PIPE 방식은 근본적으로 결국 일종의 임시 파일을 만들어 읽고 쓰는 방식이므로 본 과제의 ‘파일 시스템을 사용한 통신 금지’ 조건이 다소 우려되어 **파일 대신 메모리를 공유 매개체로 사용하는 IPC 방식**을 사용하고자 했습니다. 이 역시 IPv6 서버가 IPv4 클라이언트로 토큰을 전달하기만 하면 되는 단방향 통신이므로 **shared memory 방식보다 간편한 Message queue 방식을 택했습니다.** 
	- `sys/ipc.h`, `sys/msg.h`, `sys/types.h` 총 3개의 헤더파일이 필요하며, data type을 담은 long 변수와 내용을 담은 char 배열로 구성된 구조체로 통신합니다. 본 과제의 경우 전체적인 과정은 아래와 같습니다:
		1. 자식 프로세스로부터 PIPE로 토큰을 수신한 부모 프로세스는 이를 tokens라는 버퍼에 저장합니다.
		2. 데이터를 전송하는 측인 IPv6 서버의 부모 프로세스는 msgget 함수를 통해 고유한 key 값을 지정하고 message queue를 생성한 후 해당 queue의 id를 받습니다.
		3. msgsnd() 함수는 이 id를 통해 메세지 큐에 접근하고, msg_data 구조체를 만들어 message type을 지정하는 long 변수와 내용을 담은 char 배열로 구성된 구조체의 형태로 메세지를 보내야 합니다. 이 때 유의할 점은 msgsnd의 파라미터 중 msgsz는 메시지 큐에 전송할 데이터의 사이즈를 의미하는데, 위의 구조체에서 long 변수의 크기는 제외하고 실데이터의 크기만 전달해야 합니다. 본 과제에서 IPv6 서버는 넉넉히 256바이트 만큼의 버퍼를 만들어, 앞서 tokens 버퍼에 저장되어 있던 토큰 값들을 memcpy()로 복사하여 갖습니다.
		4. 데이터를 수신받는 측인 IPv4 클라이언트는 마찬가지로 msgget 함수를 사용하여 고유한 key 값으로 message queue의 id를 받아 오고, msgrcv() 함수를 통해 해당 id를 갖는 queue에 접근하여 그 안의 데이터를 메세지 구조체 내의 char 배열 크기만큼만 불러와 동일한 형태의 구조체에 저장합니다.
		5. Message queue 통신이 끝나면 IPv4 클라이언트는 memcpy()로 이 구조체의 char 배열 멤버 크기 만큼의 데이터를 recvtoken 버퍼에 저장하였습니다. 이후 해당 버퍼의 문자열 맨 끝에 0x0a를 추가하여 IPv4 서버로 전송하였습니다.

### 3\) IPv4 클라이언트가 전달받은 토큰 5개를 IPv4 서버에 전송
- 전송 시 데이터의 마지막임을 알리는 문자(혹은 숫자) 추가 필요

<br/>

## 4. Result
<img src="https://user-images.githubusercontent.com/94846990/146135823-73d8871f-9dd0-4dcc-ac7c-19ffd081af34.png">

<br/>

## 5. Unique Experience
### 1) Operating the Server behind VM or NAT
- IPv6 서버를 구현하고 IPv6 클라이언트로부터 토큰을 수신하는 과정에서, 완전히 구현했다고 생각했음에도 불구하고 accept 요청이 오지 않는 문제를 겪었습니다. 
	- 그래서 첫 번째로 고려한 원인은 포트포워딩 여부였습니다. 서버가 VM 위에서 구동되고 있는 동시에 NAT 공유기에서 할당된 사설 IP를 사용하고 있었기 때문입니다.
	- VM 포트포워딩 관련 자료를 찾아보았으나 본 서버가 IPv6주소를 갖는다는 점에서 기존 대부분의 자료들과는 차이점이 있어 바로 적용하기 어려웠습니다. 그래서 정확히 어떤 포트와 IP주소를 포워딩해주어야 하는지 파악하기 위해, 이번 수업을 통해 배운 도구인 Wireshark를 사용하여 직접 터널링된 IPv6 클라이언트가 다른 IPv6 서버에 접속하는 과정의 패킷을 잡아 확인하였습니다. 
	- 그랬을 때 실질적인 통신은 IPv4 endpoint들 사이에서 이루어지는 점을 알 수 있었고, IPv6 포트포워딩으로 인한 문제가 아님을 확인했습니다. 
	- 다만 IPv4 주소에 대해서는 포트포워딩을 해주어야 하는지 여부가 여전히 해결되지 않고 남아있었는데, 이는 Wireshark 상으로 확인했을 때 IPv4 통신이 Teredo 서버와 저의 NAT 외부 공인IP 사이에서 이루어지고 있었기 때문입니다.

- IPv6 터널링의 원리를 추가로 공부하며, IPv6 호스트끼리 직접 통신하는 것이 아니라 IPv6 데이터그램이 IPv4 패킷에 캡슐화된 형태로 IPv4 호스트 사이에 전달된다는 것을 알 수 있었습니다.
	- 그래서 여전히 IPv6 서버가 accept 요청을 받지 못하는 것에 대해 IPv4 공인IP로 수신된 패킷의 IPv6 데이터그램 캡슐 부분이 제 VM에 할당되어 있는 사설IP로 제대로 포워딩되지 않고 있다고 생각했습니다. 그러나 저의 IPv4 호스트가 Teredo 서버와 어떤 포트로 통신하고 있는지 알 수 없었기 때문에 포트포워딩 설정에 어려움을 겪었습니다. 
	- 추후 이 부분이 문제가 아니었음을 확인했지만, 이 과정에서 IPv6 터널링의 원리를 이해하고 직접 IPv6, IPv4 인터페이스의 패킷을 각각 잡아보며 실제 통신이 구현되는 과정을 파악할 수 있었습니다. 또한 위의 과정에서 IPv4 포트포워딩을 위한 다음의 몇 가지 방법들을 추가로 공부할 수 있었습니다.
- **`VMware bridge mode`** : Host OS의 네트워크 어댑터와 직접 연결된 것처럼 통신하는 방식입니다. 
	- VMware는 기본적으로 네트워크를 NAT 모드로 설정하는데, 이로 인해 Host PC로부터 IP를 다시 할당 받아서 VMware가 자체 DHCP 서버를 띄워 내부 네트워크 대역을 할당하고 통신하고 있었습니다. 
	- 그러나 외부 네트워크와 통신하려면 반드시 Host PC를 통해야 해서, 포트포워딩을 이중으로 해주어야 하는지 헷갈렸습니다. 그래서 아예 VMware(Guest PC)도 공유기의 사설 IP 네트워크에 직접 속할 수 있도록 공유기로부터 사설 IP를 할당받는 bridge 모드를 사용하였습니다.
- **`TWIN IP / DMZ`** : 앞선 bridge 모드를 사용했음에도 불구하고 여전히 토큰이 수신되지 않았기 때문에, 문제의 원인이 사설 IP에 있다고 착각하여 VMware가 임시로라도 공인IP를 사용할 수 있는 방법이 있는지 찾아보았습니다. 이 과정에서 공유기의 DMZ와 TWIN IP 기능을 시도했습니다. 
	- 우선 앞서 IPv4 호스트가 Teredo 서버와 어떤 포트로 통신하고 있는지 알 수 없는 문제에 대해, 모든 포트를 개방하는 DMZ를 사용하여 포워딩을 시도했습니다. 이는 포트포워딩보다 훨씬 설정이 쉽다는 장점이 있었습니다. 
	- 다만 모든 포트를 개방하면 보안에 취약해진다는 단점이 있으므로 본 과제와 같이 개인적인 작은 프로젝트에서는 가능하지만 대규모 서비스 혹은 기업체의 경우 사용에 많은 위험이 따를 수 있는 방법이라고 생각합니다. 이번 경우에는 공유기의 네트워크망에 연결된 기기가 노트북과 핸드폰, VMware 정도가 전부였으므로 단점은 크게 고려하지 않았습니다. 
	- Twin IP의 경우, 아예 공인 IP로 인식되기 때문에 서버 구축시 사용된다고 하여 시도해 보았습니다. 
	- 앞선 DMZ는 공유기의 모든 패킷이 포워딩되긴 하지만 IP 자체는 여전히 사설 IP로 인식되는 데 반해, Twin IP를 사용하면 아예 공인 IP로 인식될 수 있습니다.

### 2) IPC after fork function
- IPC는 IPv6 서버 내에서 자식-부모 프로세스 간에 한 번, 가상머신 내에서 IPv6 서버와 IPv4 클라이언트 간에 한 번으로 총 두 번을 구현해야 했습니다. 
	- 우선 자식 프로세스 내에서 token이라는 버퍼를 만들어 수신된 토큰을 저장하고 부모 프로세스에서 다시 token이라는 버퍼를 호출하여 저장되어 있는 데이터(토큰)을 출력하려 했을 때 아무것도 출력되지 않았습니다. 
	- Fork() 함수로 분리된 자식 프로세스와 부모 프로세스는 각기 독립적인 메모리 공간을 할당받기 때문에, 같은 이름의 버퍼이더라도 두 프로세스에서 각각 아예 다른 메모리 주소를 갖고 있다는 것을 미처 고려하지 못했습니다.
- 그래서 자원을 복사하지 않고 자식 프로세스가 부모 프로세스의 자원을 공유받아 갖게 되는 vfork() 함수를 대신 사용하고자 했으나, 부모 프로세스와 자식 프로세스가 각각 다른 소켓을 닫아야 했기 때문에 해당 함수를 사용하면 두 프로세스에서 닫히는 소켓이 동일해진다는 문제가 발생했습니다. 
	- 본 과제의 경우 자식 프로세스가 수신한 토큰을 일방적으로 부모 프로세스에게 넘겨주기만 하면 되는 일방향 통신이므로 간단히 PIPE 방식을 사용하였습니다. 
	- 동일한 PPID를 가진 프로세스들, 즉 부모-자식 프로세스 간 통신에 사용되는 PIPE는 half-duplex flow를 갖습니다. 
	- read()와 write()는 기본적으로 block 모드로 작동하기 때문에, full duplex를 구현하기 위해서는 PIPE 파일이 두 개 필요합니다. 

<br/>

## 6. Further Discussion
- IPv6 서버의 부모 프로세스가 모든 토큰 수신이 끝난 후 `listen()`에 사용되었던 소켓을 닫아주지 못했다(`close(listen_sock_fd)` 필요). 제출이 끝나고 알아차려서 너무 아쉽다😭
- trials의 IPv6서버 코드와 최종 제출한 IPv6 서버 코드와의 차이점은 <u>(1) 자식 클라이언트가 `if (pid==0)` 문 안에서 `listen`에 사용된 소켓을 닫느냐 닫지 않느냐</u> 하는 부분과 <u>(2) 자식 클라이언트가 `accept`된 소켓으로부터 토큰을 읽어올 때 `read()` 함수로 그냥 한 번 읽어오느냐 또는 읽을 바이트가 없을 때까지 `while`문으로 읽어오느냐</u> 하는 부분의 두 가지 뿐인데, trials의 코드는 작동하지 않았어서 정확히 어느 부분이 문제를 발생시켰는지 제대로 파악하고 싶다.
- 소켓 프로그래밍인 만큼 IPv6 서버와 IPv4 클라이언트 간의 통신도 소켓으로 해보고 싶었는데 머리가 복잡해져서 그러지 못했다. 이미 하나의 소켓이 열려서 다른 프로그램과 통신하고 있을 때 또다른 소켓을 열어 하나의 통신을 더 하는 방법을 공부해봐야 할 듯 싶다.
